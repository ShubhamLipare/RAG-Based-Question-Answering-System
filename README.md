# RAG-Based Question Answering System

## ğŸ“Œ Overview
This project implements a **Retrieval-Augmented Generation (RAG)** based chatbot using **LangChain**, **FastAPI**, **Streamlit**, **FAISS/ChromaDB**, and **MLflow**. It combines a **retriever** (vector database) with a **generator** (LLM-based response generation) to answer user queries effectively.

## ğŸ—ï¸ Project Structure
```
RAG-Based-Chatbot/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api.py  # FastAPI backend
â”‚   â”œâ”€â”€ generator/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ exception.py
â”‚â”€â”€ ui/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py  # Streamlit frontend
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
```

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-repo/rag-chatbot.git
cd rag-chatbot
```

### 2ï¸âƒ£ Create and Activate Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate  # Windows
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set Up MLflow Tracking Server
```bash
mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 0.0.0.0 --port 5000
```

### 5ï¸âƒ£ Start FastAPI Server
```bash
uvicorn src.api.api:app --host 0.0.0.0 --port 8000 --reload
```

### 6ï¸âƒ£ Start Streamlit UI
```bash
streamlit run src/ui/app.py
```

## ğŸ› ï¸ How It Works
1. **Data Ingestion**: Loads and preprocesses the documents.
2. **Vector Store**: Converts text into embeddings and stores them in FAISS/ChromaDB.
3. **Query Processing**: Retrieves relevant chunks using vector similarity search.
4. **Response Generation**: Uses a language model (LLM) to generate contextual responses.

## ğŸ“¡ API Endpoints
| Method | Endpoint  | Description |
|--------|----------|-------------|
| `GET`  | `/chat`  | Generates a chatbot response based on a query |

### Example API Call
```bash
curl "http://127.0.0.1:8000/chat?query=Tell me about RAG"
```

## ğŸ–¥ï¸ Running the Application
- Open **[http://127.0.0.1:8501](http://127.0.0.1:8501)** in your browser for the Streamlit UI.
- Enter your query and interact with the chatbot.

## ğŸ”¥ Troubleshooting
### âŒ FastAPI not running
- Run `uvicorn src.api.api:app --host 0.0.0.0 --port 8000 --reload`
- Check if port **8000** is occupied (`netstat -ano | findstr :8000`)

### âŒ Streamlit UI not loading
- Check logs for errors (`streamlit run src/ui/app.py`)
- Ensure FastAPI is running first

### âŒ MLflow errors
- Ensure MLflow is running (`mlflow server --host 0.0.0.0 --port 5000`)
- Set tracking URI correctly in `api.py`

## âœ¨ Future Enhancements
- ğŸ”¹ Deploy API using **Docker & Kubernetes**
- ğŸ”¹ Add **authentication & user history tracking**
- ğŸ”¹ Fine-tune **LLM for domain-specific responses**

---
### ğŸ¯ Contributors
- **Your Name** - Developer
- **Your Team** - Collaborators

ğŸ“© **Have issues?** Open an issue or reach out!

---
ğŸ”— **GitHub Repository:** [Your Repo](https://github.com/your-repo/rag-chatbot)

